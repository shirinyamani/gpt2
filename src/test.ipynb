{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "gpt2_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") #initiating the init file of gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "sd_gpt2 = gpt2_hf.state_dict()\n",
    "for k,v in sd_gpt2.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7852)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_gpt2[\"transformer.wte.weight\"].view(-1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1101, -0.0393,  0.0331,  0.1338, -0.0485, -0.0789, -0.2398, -0.0895,\n",
       "         0.0253, -0.1074])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_gpt2[\"transformer.wte.weight\"].view(-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-21 13:27:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-09-21 13:27:56 (11.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file to inspect\n",
    "with open(file='./data/input.txt', mode='r',  encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique chars:['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "f'len text:{len(text)}'\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'number of unique chars:{chars}')\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization \n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "#enocde\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "#decode\n",
    "decode = lambda numz: ''.join([itos[i] for i in numz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "#encode('hi there!')\n",
    "print(decode(encode('hi there!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "#encode entire data \n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype= torch.long)\n",
    "\n",
    "#same bc special case of having no dimension \n",
    "test = data.view(-1)[:1000] #self.long() is equivalent to self.to(torch.int64).\n",
    "print(test.shape)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train/test\n",
    "n = int(0.9 * len(data))\n",
    "train_set = data[:n]\n",
    "test_set = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_set[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when context is tensor([18]) we predict 47\n",
      "when context is tensor([18, 47]) we predict 56\n",
      "when context is tensor([18, 47, 56]) we predict 57\n",
      "when context is tensor([18, 47, 56, 57]) we predict 58\n",
      "when context is tensor([18, 47, 56, 57, 58]) we predict 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1]) we predict 15\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15]) we predict 47\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47]) we predict 58\n"
     ]
    }
   ],
   "source": [
    "#data loader \n",
    "#time dimension\n",
    "x = train_set[:block_size]\n",
    "y = train_set[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #to include t \n",
    "    target= y[t]\n",
    "    print(f'when context is {context} we predict {target}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch size is torch.Size([4, 8])\n",
      "the x_batch is: \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "========================================\n",
      "target y_batch size is torch.Size([4, 8])\n",
      "the x_batch is: \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "========================================\n",
      "when the context is [24] we are predicting 43\n",
      "when the context is [24, 43] we are predicting 58\n",
      "when the context is [24, 43, 58] we are predicting 5\n",
      "when the context is [24, 43, 58, 5] we are predicting 57\n",
      "when the context is [24, 43, 58, 5, 57] we are predicting 1\n",
      "when the context is [24, 43, 58, 5, 57, 1] we are predicting 46\n",
      "when the context is [24, 43, 58, 5, 57, 1, 46] we are predicting 43\n",
      "when the context is [24, 43, 58, 5, 57, 1, 46, 43] we are predicting 39\n",
      "when the context is [44] we are predicting 53\n",
      "when the context is [44, 53] we are predicting 56\n",
      "when the context is [44, 53, 56] we are predicting 1\n",
      "when the context is [44, 53, 56, 1] we are predicting 58\n",
      "when the context is [44, 53, 56, 1, 58] we are predicting 46\n",
      "when the context is [44, 53, 56, 1, 58, 46] we are predicting 39\n",
      "when the context is [44, 53, 56, 1, 58, 46, 39] we are predicting 58\n",
      "when the context is [44, 53, 56, 1, 58, 46, 39, 58] we are predicting 1\n",
      "when the context is [52] we are predicting 58\n",
      "when the context is [52, 58] we are predicting 1\n",
      "when the context is [52, 58, 1] we are predicting 58\n",
      "when the context is [52, 58, 1, 58] we are predicting 46\n",
      "when the context is [52, 58, 1, 58, 46] we are predicting 39\n",
      "when the context is [52, 58, 1, 58, 46, 39] we are predicting 58\n",
      "when the context is [52, 58, 1, 58, 46, 39, 58] we are predicting 1\n",
      "when the context is [52, 58, 1, 58, 46, 39, 58, 1] we are predicting 46\n",
      "when the context is [25] we are predicting 17\n",
      "when the context is [25, 17] we are predicting 27\n",
      "when the context is [25, 17, 27] we are predicting 10\n",
      "when the context is [25, 17, 27, 10] we are predicting 0\n",
      "when the context is [25, 17, 27, 10, 0] we are predicting 21\n",
      "when the context is [25, 17, 27, 10, 0, 21] we are predicting 1\n",
      "when the context is [25, 17, 27, 10, 0, 21, 1] we are predicting 54\n",
      "when the context is [25, 17, 27, 10, 0, 21, 1, 54] we are predicting 39\n"
     ]
    }
   ],
   "source": [
    "#make use of prallalism; So a couple of same-size blocks \n",
    "#bTCH Dimension\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # independent sequences to be processed in parallel every forward/backward \n",
    "block_size = 8 #maximum context len in each of the sequence, can be less than block size! in each of the predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_set if split == \"train\" else test_set\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i + block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x_batch, y_b = get_batch(split='train')\n",
    "print(f'input batch size is {x_batch.shape}')\n",
    "print(f'the x_batch is: \\n {x_batch}')\n",
    "\n",
    "print('=='*20)\n",
    "\n",
    "print(f'target y_batch size is {y_b.shape}')\n",
    "print(f'the x_batch is: \\n {y_b}')\n",
    "\n",
    "print('=='*20)\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension\n",
    "        context = x_batch[b, :t+1]\n",
    "        target = y_b[b, t]\n",
    "        print(f'when the context is {context.tolist()} we are predicting {target}')\n",
    "\n",
    "# 32 independent exp packed in a single batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(x_batch) #input to the transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline: bigram \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" nn.Embedding is a wrapper\n",
    "        input: list of indices.\n",
    "        out: corresponding word embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #wrapper \n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size) \n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        logits = self.token_embeding_table(idx) #(B,T, C)\n",
    "        #print(logits.shape)\n",
    "        if target is None:\n",
    "            loss = None \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #streched \n",
    "            # print(f'logits shape is {logits.shape} and it looks like:')\n",
    "            # print(logits)\n",
    "            target = target.view(B*T) #-1 streched \n",
    "            # print(f'logits shape is {target.shape} and it looks like:')\n",
    "            # print(target)\n",
    "            loss = F.cross_entropy(logits, target) #should be (B*T,C)  # LOSS: -ln(1/65)\n",
    "            #print(f'idx shape is {idx.shape}')\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx shape is (B, T) which is the current context within some batch\n",
    "        #so the job of generate is to take the (B,T) and extend it to (B, T + 1) in all the time dimensions within the batches\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get predictions\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:] #becomes (B, C)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
    "            #sample from this probs\n",
    "            # If input is a matrix with m rows, out is an matrix of shape (m * num_samples)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B * 1) \n",
    "            #appending the sample to the current seq\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "#inspect forward pass     \n",
    "obj = BigramLanguageModel(vocab_size)\n",
    "logits, loss = obj(x_batch, y_b)\n",
    "print(logits.shape)\n",
    "print(loss) \n",
    "# idx is (B,T) , generate works on B then we gotta unplog the created one [0]\n",
    "# zeros was \\n \n",
    "print(decode(obj.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4021,  0.7641],\n",
      "         [-1.0255, -1.2937],\n",
      "         [ 0.6774, -1.0308],\n",
      "         [-0.4239,  0.6698]],\n",
      "\n",
      "        [[ 0.6774, -1.0308],\n",
      "         [ 0.3770,  2.2209],\n",
      "         [-1.0255, -1.2937],\n",
      "         [-0.4239,  0.6698]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#inspect nn.Embedding\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 5]])\n",
    "embed = nn.Embedding(6, 2)\n",
    "print(embed(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now train the model \n",
    "optim = torch.optim.AdamW(obj.parameters(),  lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in step 999 loss is: 2.4415125846862793\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    # get a sample batch \n",
    "    x_batch, y_b = get_batch(split='train')\n",
    "    \n",
    "    #forward/backward\n",
    "    # evaluate the loss\n",
    "    logits, loss = obj( x_batch, y_b)\n",
    "    #zero out all the gradients from prev steps \n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    #gettting the gradients for all the params \n",
    "    loss.backward()\n",
    "    #using those gradients to update our params\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "print(f' in step {steps} loss is: {loss.item()}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "US lawevengoft tho mooreawe he m!\n",
      "\n",
      "This ghespins mainourerkenowoverch t HAne inong. wighe?\n",
      "I:\n",
      "RDUg, \n"
     ]
    }
   ],
   "source": [
    "print(decode(obj.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "att me I y?\n",
      "Footo! has wh-OMar be hots. VEObllan\n",
      "Plouls plesthen ther a be wean I pr itrg t per mat.\n",
      "y to f iserd me l ms moe!\n",
      "BO,\n",
      "ELe.\n",
      "LI thir:\n",
      "Watheas n:\n",
      "MI's'd, V:\n",
      "\n",
      "I.\n",
      "T:\n",
      "Thfer, t nt hore m I rathoue hethy,-tshy ly.\n",
      "YBE:\n",
      "Hamathe doulond wha my\n",
      "Y:\n",
      "De.\n",
      "Cor y my?\n",
      "SCKERUThy&xqupacoor l it igr mes ownd y?-esageqThatou bethour t\n",
      "\n",
      "Aprisl bicther d I borchiordugovand nepellithipamer warmerur and mprothy w! Lirilmowobegrileaviere fougr nethealvith a gouromat my y panghesthe?\n",
      "ueaye haly tokersos ck. bu\n"
     ]
    }
   ],
   "source": [
    "print(decode(obj.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attention inspection\n",
    "#toy exp\n",
    "import torch \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version1: Average type interacrion of the tokens up to current token as the simpleset possible interaction way\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #including current token (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) #over zero dim which id time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 2; using matrix mul\n",
    "# back up for inspecting the matrix mul\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1 ,keepdim=True)\n",
    "xbow2 = wei @ x #(T,T) * (B, T, C) ==> (B, T, C) broadcasting in pytorch the T,T will be B,T,T then for every batch \n",
    "# it happens on the level of batch, matrix mul on each of the batches in parallel and indivisually \n",
    "torch.allclose(xbow[0], xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3: using softmx\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = tril.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wbow3 = wei @ x\n",
    "torch.allclose(xbow[0], wbow3[0])\n",
    "\n",
    "#TAKEOUT: we can do weighted aggregation of the past elements by Matrix Mul of the lower tril and the elements in this lower tril tells us how much this element is fuses in this positon!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = a @ b\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3)) \n",
    "a = a/ torch.sum(a, 1, keepdim=True)    #normalizing a: all rows sum to 1 ~ softmax \n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b # (3*3) (3,2) => 3*2\n",
    "\n",
    "\n",
    "print(f'a=')\n",
    "print(a)\n",
    "print(f'b=')\n",
    "print(b)\n",
    "print(f'c = a @ b')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect difference nn.Linear vs nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=2, bias=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Linear(10, 2)\n",
    "a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on difference nn.Linear and nn.Embedding \n",
    "\n",
    "An Embedding layer is essentially just a Linear layer. So you could define a your layer as nn.Linear(1000, 30), and represent each word as a one-hot vector, e.g., [0,0,1,0,...,0] (the length of the vector is 1,000).\n",
    "\n",
    "As you can see, any word is a unique vector of size 1,000 with a 1 in a unique position, compared to all other words. Now giving such a vector v with v[2]=1 (cf. example vector above) to the Linear layer gives you simply the 2nd row of that layer.\n",
    "\n",
    "nn.Embedding just simplifies this. Instead of giving it a big one-hot vector, you just give it an index. This index basically is the same as the position of the single 1 in the one-hot vector.\n",
    "\n",
    "\n",
    "\n",
    "‘nn.Embedding’ is no architecture, it’s a simple layer at best. In fact, it’s a linear layer just with a specific use.\n",
    "\n",
    "Internally, nn.Embedding is – like a linear layer – a M x N matrix, with M being the number of words and N being the size of each word vector. There’s nothing more to it. It just matches a word (specified by an index) to the corresponding word vector, i.e., the corresponding row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "#inspect on nn.Linear\n",
    "m = nn.Linear(16, 16)\n",
    "input = torch.randn(2, 3, 16)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2935e-01, -5.4255e-02,  8.6804e-01,  7.1032e-01,  6.4082e-01,\n",
       "           1.2002e+00,  7.3290e-01,  9.9904e-01,  3.0008e-01, -2.6067e-01,\n",
       "          -1.5186e-02, -3.1486e-01, -1.2295e+00,  4.9245e-02,  1.2247e-02,\n",
       "           8.3614e-01],\n",
       "         [-1.0657e+00, -1.0875e-01, -2.2117e-02,  1.2596e+00,  1.0819e+00,\n",
       "          -5.9091e-01, -1.0574e+00, -1.2076e+00,  5.5400e-01,  2.9481e-01,\n",
       "          -6.0595e-01, -1.8302e+00, -1.1210e+00, -8.2700e-01, -2.6840e-01,\n",
       "          -7.9106e-01],\n",
       "         [ 2.4123e-01, -1.8315e-01, -5.6941e-02,  5.0335e-01, -3.5398e-01,\n",
       "          -5.4684e-01, -3.5876e-01, -5.8759e-01, -4.7856e-01,  2.5169e-02,\n",
       "           1.4939e-01, -4.3014e-01, -1.4933e-01, -3.4374e-01,  7.2644e-01,\n",
       "           2.4808e-02]],\n",
       "\n",
       "        [[-2.8538e-01,  6.1068e-01,  4.4531e-01,  6.3977e-01, -3.6194e-01,\n",
       "          -9.6942e-02, -1.1857e+00, -6.5978e-01, -8.1429e-01,  7.4203e-01,\n",
       "          -6.2159e-01, -4.8497e-01, -1.4999e-01, -3.4138e-01, -4.7673e-04,\n",
       "          -9.8899e-02],\n",
       "         [ 6.7898e-01, -8.9714e-01,  6.1797e-01,  2.7415e-01, -6.7352e-01,\n",
       "          -4.3593e-01,  4.0863e-01,  1.1130e+00, -1.0145e-01, -3.1762e-01,\n",
       "          -1.4695e-01,  3.9966e-02, -8.3210e-01, -8.0419e-01,  4.3601e-01,\n",
       "           5.6128e-01],\n",
       "         [ 1.4365e-01,  4.0391e-01,  5.8929e-01, -1.0005e+00,  8.7535e-01,\n",
       "          -8.0267e-01,  1.2124e+00, -5.1299e-01,  3.6598e-01, -8.7065e-01,\n",
       "           8.9459e-01,  1.0206e+00, -5.3281e-01,  1.3837e-01, -8.7596e-01,\n",
       "           3.2164e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 4: Self-aatention: differece w/ version 3, is the weights instead of uniformely distributed, should be data dependent\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.rand(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "#each token in the B,T fashion will emit 2 vectors \n",
    "key = nn.Linear(C, head_size, bias=False) #apply matrix mul\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B, T, head_size\n",
    "q = query(x) #  B, T, head_size\n",
    "v = value(x)\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, head_size) * (B, head_size, T) ==> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "#out = wei # x\n",
    "out = wei @ v # (B, T, head_size) 4,8,16\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],\n",
       "        [0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],\n",
       "        [0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#INSPECT On sqrt head_size ** 0.5\n",
    "a = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
